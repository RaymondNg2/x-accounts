* First, really simple implementation. The whole thing runs in about 1.6 seconds. No concurrency, although I added synchronisation from the start.
* Added threads to run the code. That slows things down a bit. 2.2 seconds now seems to be the norm.
* I assume lock contention is the current bottleneck. I split the large lock on balance into a lock on the from and to entries on the balance array. That brings the time down to 1.8 seconds. Nice, good guess.
* Crap, I found a bug in my test case. I missed an indirection when selecting account ID's for transfer. This adds a whole second(!) to my times: 2.7 seconds. The good news is that this adds to the time taken by the test itself, not my code executing the actual logic.
* Added HTTP for all methods. The entire test now runs in 213 seconds, or about 20.000 transactions per second. Checking for the 25ms cut-off time shows that an awful lot of calls don't make that deadline. Further tuning ahead.
* Switched from uni-rest to using the bare HTTPUrlConnection from the JDK. This is about 10% faster (22.000 transactions per second) but results in occasional "java.net.SocketException: Invalid argument" messages that screw up the tests. Not sure this is a valid path. Need to fiddle a little more before I abandon it.
* Tweaked the HTTP libraries with -Dhttp.maxConnections=110. That reduces the number of actual HTTP connections being set up and avoids the dreaded "java.net.SocketException: Invalid argument" messages. JProfiler tells me that JSON parsing on the server side is significant overhead (~6% cpu) but still Jetty us the big server-side hog (~25% cpu). Note that these have client and server still running in a single VM.
* Split the server into a separate JVM on my laptop. That does not affect timing in any way, but it does give me a better view of where the server spends its time. JSON parsing is a significant contributor (~60% cpu).
* Replaced the JSON parser with a crude string scanner that uses a *lot* of assumptions about the JSON messages to speed up parsing. This gives us an extra 10% transactions per second. We are now up at 24.000 transactions per second on my Macbook Air.
* Using ngrep I have rough sizes of network traffic. From there I calculated the network throughput of an m3.large Amazon EC2 instance. Once we can run the test in under a minute, raw network throughput is going to be the bottleneck.
    POST /account:  request 236 bytes, response 140 bytes (may be trimmed to 70 bytes)
    POST /transfer: request 263 bytes, response 141 bytes (may be trimmed to 71 bytes)
     300301 accounts of  236+140 bytes each is  112913176 bytes is about  110 MB
    3899999 transfers of 263+141 bytes each is 1575599596 bytes is about 1502 MB
    m3.large is rated at ~700 Mbit/s (http://stackoverflow.com/questions/18507405/ec2-instance-typess-exact-network-performance)
    in bc(1) syntax:
        (((263+71) * 3899999) + ((236+140) * 300301)) / (700*1024*1024/8)
    thus, one m3.large can handle all traffic in about 20 seconds
* Found a problem with the API that I made. Return values were wrong, which I now fixed. As part of the fix I have to make two calls for each transfer, not just one. One to enqueue the transfer and one to read the result. As expected this more or less halves the amount of transactions to 11.500 per second.
* As an experiment I configured Jetty to suppress the "Server" header on its HTTP responses. That saves some 20 bytes on each HTTP response. That may not sound like much, but on the whole test that is about 150 MB, or 5% of all traffic. Not sure it makes a difference. The tests seem to be running into GC and memory issues on the test driver side. I see multi-second pauses there.
* Implemented a simple web server, to reduce HTTP parsing overhead that Jetty has. Obviously not code that is useful anywhere but for this one implementation of this one challenge.
* Switched back to Unirest on the HTTP client. Turns out that Java's HTTPURLConnection has serious contention issues on the internal KeepAliveCache. Since we are doing so many concurrent requests caused the client code to essentially be blocked on the KeepAliveCache all the time. See the screenshots from JProfiler I took on that subject. By reverting to Unitrest we can now do about 17.000 transactions per second again. Important is setting the concurrency, so that Unirest uses more concurrent connections.
