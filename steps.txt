* First, really simple implementation. The whole thing runs in about 1.6 seconds. No concurrency, although I added synchronisation from the start.
* Added threads to run the code. That slows things down a bit. 2.2 seconds now seems to be the norm.
* I assume lock contention is the current bottleneck. I split the large lock on balance into a lock on the from and to entries on the balance array. That brings the time down to 1.8 seconds. Nice, good guess.
* Crap, I found a bug in my test case. I missed an indirection when selecting account ID's for transfer. This adds a whole second(!) to my times: 2.7 seconds. The good news is that this adds to the time taken by the test itself, not my code executing the actual logic.
* Added HTTP for all methods. The entire test now runs in 213 seconds, or about 20.000 transactions per second. Checking for the 25ms cut-off time shows that an awful lot of calls don't make that deadline. Further tuning ahead.
* Switched from uni-rest to using the bare HTTPUrlConnection from the JDK. This is about 10% faster (22.000 transactions per second) but results in occasional "java.net.SocketException: Invalid argument" messages that screw up the tests. Not sure this is a valid path. Need to fiddle a little more before I abandon it.
* Tweaked the HTTP libraries with -Dhttp.maxConnections=110. That reduces the number of actual HTTP connections being set up and avoids the dreaded "java.net.SocketException: Invalid argument" messages. JProfiler tells me that JSON parsing on the server side is significant overhead (~6% cpu) but still Jetty us the big server-side hog (~25% cpu). Note that these have client and server still running in a single VM.
